{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [LangChain Quickstart Guide](https://python.langchain.com/docs/get_started/quickstart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the environmental variables form .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import textwrap\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first: `pip install langchain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printlongtext(text, max_lines=10):\n",
    "    short_strings = textwrap.wrap(text, 80)\n",
    "    lines = len(short_strings)\n",
    "    if lines < max_lines:\n",
    "        max_lines = lines\n",
    "    for index in range(0, max_lines):\n",
    "        print(short_strings[index])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Chain\n",
    "We first need to: `pip install langchain-openai`<br/>\n",
    "Make sure we have the openai credentials in the .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    # model=\"gpt-4\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(\n",
    "    model = \"llama2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "result =llm.invoke(\"what's the difference between langchain and llamaindex?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Langchain and Llamaindex are two different tools that serve distinct purposes in the context of natural language processing (NLP) and machine learning (ML). Here's a comparison of the two:\n",
      "\n",
      "1. Language Modeling: Langchain is primarily designed for language modeling tasks, such as text generation, language translation, and language understanding. On the other hand, Llamaindex is focused on index-based language modeling, which involves creating an index of pre-defined phrases or sentences to improve the efficiency of language processing tasks.\n",
      "2. Architecture: Langchain utilizes a transformer-based architecture, similar to BERT and RoBERTa, while Llamaindex leverages an index-based architecture that stores pre-defined phrase embeddings in a compact format.\n",
      "3. Training: Langchain requires extensive training on large datasets to learn the patterns and relationships between words in a language, while Llamaindex can be trained quickly and easily using a small dataset of pre-defined phrases.\n",
      "4. Flexibility: Langchain is more flexible and can handle a wide range of NLP tasks, such as text classification, sentiment analysis, question answering, and dialogue generation. In contrast, Llamaindex is optimized for index-based language modeling tasks like language translation and language understanding.\n",
      "5. Computational requirements: Due to its transformer-based architecture, Langchain requires significant computational resources to train and run. In contrast, Llamaindex can be trained and deployed more efficiently due to its index-based approach, which reduces the computational requirements.\n",
      "6. Interpretability: Langchain provides more interpretability compared to Llamaindex since it uses a transformer architecture that generates attention weights for each word in a sequence. This allows researchers and developers to understand which words are most important for the model's predictions. In contrast, Llamaindex relies on pre-defined phrase embeddings, making it more challenging to interpret the model's internal workings.\n",
      "7. Scalability: Langchain can handle larger datasets and more complex NLP tasks than Llamaindex due to its transformer architecture. However, Llamaindex is optimized for efficient deployment on resource-constrained devices, making it a better choice for applications that require real-time language processing capabilities.\n",
      "8. Initialization: Langchain requires initialization using a large corpus of text data, while Llamaindex can be initialized with a small dataset of pre-defined phrases.\n",
      "9. Continuous learning: Langchain can learn continuously from new text data as it becomes available, while Llamaindex is designed for static language modeling tasks and does not support continuous learning.\n",
      "10. Specialized tasks: Langchain is better suited for specialized NLP tasks like language translation, question answering, and dialogue generation, while Llamaindex is optimized for index-based language modeling tasks.\n",
      "\n",
      "In summary, Langchain and Llamaindex serve different purposes in the context of NLP and ML. While Langchain provides more interpretability and flexibility, Llamaindex offers faster deployment and lower computational requirements, making it a better choice for certain applications.\n"
     ]
    }
   ],
   "source": [
    "# print(result.content)\n",
    "print(result)\n",
    "# print the json object as a string with pretty formatting with indent of 4, also make it wrap long lines\n",
    "# print(json.dumps(result.json(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine prompt and llm into a chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langsmith can help with testing in several ways:\\n\\n1. Automation: Langsmith provides powerful automation tools and frameworks that can help automate testing processes, such as test case execution, regression testing, and performance testing. This can save time and effort in the testing process and improve efficiency.\\n\\n2. Test data generation: Langsmith can generate realistic and diverse test data that can be used to validate the functionality and performance of the software under test. This can help in identifying potential issues and improving test coverage.\\n\\n3. Test case management: Langsmith offers test case management capabilities that allow testers to organize, prioritize, and track test cases effectively. This can streamline the testing process and ensure that all necessary tests are executed.\\n\\n4. Reporting and analytics: Langsmith provides comprehensive reporting and analytics features that can help testers analyze test results, identify trends, and make data-driven decisions. This can improve the overall quality of the software being tested.\\n\\n5. Integration with other tools: Langsmith can integrate with other testing tools and systems, such as continuous integration and delivery (CI/CD) pipelines, defect tracking systems, and monitoring tools. This seamless integration can enhance the testing workflow and improve collaboration among team members.\\n\\nOverall, Langsmith can be a valuable asset in the testing process by providing automation, test data generation, test case management, reporting and analytics, and integration capabilities.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is a powerful tool that can greatly assist with testing in a variety of ways. Here are some ways in which Langsmith can help with testing:\n",
      "\n",
      "1. **Automated Testing:** Langsmith can be used to automate testing tasks, such as running tests, comparing output, and generating reports. This can help streamline the testing process and ensure more consistent and thorough testing coverage.\n",
      "\n",
      "2. **Test Data Generation:** Langsmith can generate a wide variety of test data, including strings, numbers, dates, and more. This can be especially useful for creating test cases with different input values to ensure comprehensive testing coverage.\n",
      "\n",
      "3. **Mocking and Stubbing:** Langsmith can be used to create mock objects or stubs for dependencies in your tests. This can help isolate the code under test and make testing easier by simulating the behavior of external components.\n",
      "\n",
      "4. **Performance Testing:** Langsmith can be used to simulate load and stress testing scenarios by generating a large volume of test data and measuring the performance of the system under different conditions.\n",
      "\n",
      "5. **Integration Testing:** Langsmith can help with testing the integration between different components of a system by providing tools to easily set up and tear down test environments, interact with APIs, and verify responses.\n",
      "\n",
      "Overall, Langsmith can be a valuable tool for testing teams to improve the efficiency, reliability, and effectiveness of their testing processes.\n"
     ]
    }
   ],
   "source": [
    "# result is no longer an json object, it is a string\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrival Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WebBaseLoader uses beautifulsoup to scrape text from a web page.<br/>\n",
    "We first need to: `pip install beautifulsoup4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith | 🦜️🛠️ LangSmith      Skip to main content🦜️🛠️ LangSmith DocsLangChain\n",
      "Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser\n",
      "GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt\n",
      "HubLangSmithOn this pageLangSmithIntroduction​LangSmith is a platform for\n",
      "building production-grade LLM applications.It lets you debug, test, evaluate,\n",
      "and monitor chains and intelligent agents built on any LLM framework and\n",
      "seamlessly integrates with LangChain, the go-to open source framework for\n",
      "building with LLMs.LangSmith is developed by LangChain, the company behind the\n",
      "open source LangChain framework.Quick Start​Tracing: Get started with the\n",
      "tracing quick start.Evaluation: Get started with the evaluation quick start.Next\n",
      "Steps​Check out the following sections to learn more about LangSmith:User Guide:\n",
      "Learn about the workflows LangSmith supports at each stage of the LLM\n",
      "application lifecycle.Setup: Learn how to create an account, obtain an API key,\n",
      "and configure your environment.Pricing: Learn about the pricing model for\n",
      "LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Tracing:\n",
      "Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the\n",
      "evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a\n",
      "prompt management tool built into LangSmith.Additional Resources​LangSmith\n",
      "Cookbook: A collection of tutorials and end-to-end walkthroughs using\n",
      "LangSmith.LangChain Python: Docs for the Python LangChain library.LangChain\n",
      "--------------------------------------------------------------------------------\n",
      "{\n",
      "    \"source\": \"https://docs.smith.langchain.com\",\n",
      "    \"title\": \"LangSmith | \\ud83e\\udd9c\\ufe0f\\ud83d\\udee0\\ufe0f LangSmith\",\n",
      "    \"description\": \"Introduction\",\n",
      "    \"language\": \"en\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "printlongtext(docs[0].page_content.strip(\"\\n\"), 20)\n",
    "print(\"-\" * 80)\n",
    "print(json.dumps(docs[0].metadata, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time uses a local embedding model from ollama.<br/>\n",
    "First we need to install ollama.<br/>\n",
    "Then we need to pull the model we need to use: <br/>\n",
    "`ollama pull <model_name>` <br/>\n",
    "`ollama run llama2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to do this first: `pip install faiss-cpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith can help with testing by allowing users to debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework. It provides tracing and evaluation capabilities to assist with testing the applications.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# LangSmith offers several features that can help with testing:..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith can help with testing by allowing users to debug, test, evaluate, and\n",
      "monitor chains and intelligent agents built on any LLM framework. It provides\n",
      "tracing and evaluation capabilities, as well as integration with the LangChain\n",
      "framework for building with LLMs. Users can also access the Prompt Hub, a prompt\n",
      "management tool built into LangSmith, for testing purposes.\n"
     ]
    }
   ],
   "source": [
    "printlongtext(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "result = retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith | 🦜️🛠️ LangSmith      Skip to main content🦜️🛠️ LangSmith DocsLangChain\n",
      "Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser\n",
      "GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt\n",
      "HubLangSmithOn this pageLangSmithIntroduction​LangSmith is a platform for\n",
      "building production-grade LLM applications.It lets you debug, test, evaluate,\n",
      "and monitor chains and intelligent agents built on any LLM framework and\n",
      "seamlessly integrates with LangChain, the go-to open source framework for\n",
      "building with LLMs.LangSmith is developed by LangChain, the company behind the\n",
      "open source LangChain framework.Quick Start​Tracing: Get started with the\n",
      "tracing quick start.Evaluation: Get started with the evaluation quick start.Next\n"
     ]
    }
   ],
   "source": [
    "printlongtext(result[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "result = retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith can help you test your LLM applications by providing tracing and\n",
      "evaluation capabilities. With tracing, you can debug and monitor your chains and\n",
      "intelligent agents, while evaluation allows you to test and assess the\n",
      "performance of your applications. LangSmith seamlessly integrates with\n",
      "LangChain, making it easier to build, test, and monitor your LLM applications\n",
      "effectively.\n"
     ]
    }
   ],
   "source": [
    "printlongtext(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need Tavily API Key for this. (Check .env file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool, search]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First need to: `pip install langchainhub`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith_search` with `{'query': 'how can LangSmith help with testing'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mLangSmith | 🦜️🛠️ LangSmith\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to main content🦜️🛠️ LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubLangSmithOn this pageLangSmithIntroduction​LangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the go-to open source framework for building with LLMs.LangSmith is developed by LangChain, the company behind the open source LangChain framework.Quick Start​Tracing: Get started with the tracing quick start.Evaluation: Get started with the evaluation quick start.Next Steps​Check out the following sections to learn more about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Setup: Learn how to create an account, obtain an API key, and configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources​LangSmith Cookbook: A collection of tutorials and end-to-end walkthroughs using LangSmith.LangChain Python: Docs for the Python LangChain library.LangChain Python API Reference: documentation to review the core APIs of LangChain.LangChain JS: Docs for the TypeScript LangChain libraryDiscord: Join us on our Discord to discuss all things LangChain!Contact SalesIf you're interested in enterprise security and admin features, special deployment options, or access for large teams, reach out to speak with sales.NextUser GuideIntroductionQuick StartNext StepsAdditional ResourcesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright © 2024 LangChain, Inc.\u001b[0m\u001b[32;1m\u001b[1;3mLangSmith is a platform for building production-grade LLM (Large Language Model) applications. It can help with testing by providing capabilities for debugging, testing, evaluating, and monitoring chains and intelligent agents built on any LLM framework. LangSmith seamlessly integrates with LangChain, an open-source framework for building with LLMs. \n",
      "\n",
      "Some ways LangSmith can help with testing include:\n",
      "- Tracing: LangSmith offers tracing capabilities to help you trace and debug your LLM applications.\n",
      "- Evaluation: LangSmith provides evaluation capabilities to assess the performance and effectiveness of your LLM models during testing.\n",
      "\n",
      "For more detailed information on how LangSmith can assist with testing, you can refer to the LangSmith documentation and user guide.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"input\": \"how can langsmith help with testing?\",\n",
      "    \"output\": \"LangSmith is a platform for building production-grade LLM (Large Language Model) applications. It can help with testing by providing capabilities for debugging, testing, evaluating, and monitoring chains and intelligent agents built on any LLM framework. LangSmith seamlessly integrates with LangChain, an open-source framework for building with LLMs. \\n\\nSome ways LangSmith can help with testing include:\\n- Tracing: LangSmith offers tracing capabilities to help you trace and debug your LLM applications.\\n- Evaluation: LangSmith provides evaluation capabilities to assess the performance and effectiveness of your LLM models during testing.\\n\\nFor more detailed information on how LangSmith can assist with testing, you can refer to the LangSmith documentation and user guide.\"\n",
      "}\n",
      "--------------------------------------------------------------------------------\n",
      "LangSmith is a platform for building production-grade LLM (Large Language Model)\n",
      "applications. It can help with testing by providing capabilities for debugging,\n",
      "testing, evaluating, and monitoring chains and intelligent agents built on any\n",
      "LLM framework. LangSmith seamlessly integrates with LangChain, an open-source\n",
      "framework for building with LLMs.   Some ways LangSmith can help with testing\n",
      "include: - Tracing: LangSmith offers tracing capabilities to help you trace and\n",
      "debug your LLM applications. - Evaluation: LangSmith provides evaluation\n",
      "capabilities to assess the performance and effectiveness of your LLM models\n",
      "during testing.  For more detailed information on how LangSmith can assist with\n",
      "testing, you can refer to the LangSmith documentation and user guide.\n"
     ]
    }
   ],
   "source": [
    "# print the json object as a string with pretty formatting with indent of 4, also make it wrap long lines\n",
    "print(json.dumps(result, indent=4))\n",
    "print(\"-\" * 80)\n",
    "printlongtext(result[\"output\"], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'weather in San Francisco'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'url': 'https://forecast.weather.gov/zipcity.php?inputstring=San+Francisco,CA', 'content': 'NOAA National Weather Service National Weather Service. Toggle navigation. HOME; FORECAST . Local; Graphical; Aviation; Marine; Rivers and Lakes; Hurricanes; Severe Weather; Fire Weather; ... San Francisco CA 37.77°N 122.41°W (Elev. 131 ft) Last Update: 12:26 am PST Mar 2, 2024. Forecast Valid: 1am PST Mar 2, 2024-6pm PST Mar 8, 2024 .'}, {'url': 'https://weather.com/weather/tenday/l/San Francisco CA USCA0987:1:US', 'content': \"Comfy & Cozy\\nThat's Not What Was Expected\\nOutside\\n'No-Name Storms' In Florida\\nGifts From On High\\nWhat To Do For Wheezing\\nSurviving The Season\\nStay Safe\\nAir Quality Index\\nAir quality is considered satisfactory, and air pollution poses little or no risk.\\n Health & Activities\\nSeasonal Allergies and Pollen Count Forecast\\nNo pollen detected in your area\\nCold & Flu Forecast\\nFlu risk is low in your area\\nWe recognize our responsibility to use data and technology for good. recents\\nSpecialty Forecasts\\n10 Day Weather-San Francisco, CA\\nToday\\nMon 18 | Day\\nConsiderable cloudiness. Tue 19\\nTue 19 | Day\\nLight rain early...then remaining cloudy with showers in the afternoon. Wed 27\\nWed 27 | Day\\nOvercast with rain showers at times.\"}, {'url': 'https://www.accuweather.com/en/us/san-francisco/94103/weather-forecast/347629', 'content': 'Get the current weather, radar, and air quality for San Francisco, CA. See the forecast for today, tonight, and the next two days, as well as the outlook for the next week.'}, {'url': 'https://www.wunderground.com/forecast/us/ca/san-francisco', 'content': 'Get the current and future weather conditions for San Francisco, CA, including temperature, precipitation, wind, pressure, and humidity. See historical data, maps, radar, news, and videos from Weather Underground.'}, {'url': 'https://www.accuweather.com/en/us/san-francisco/94103/current-weather/347629', 'content': 'Current weather in San Francisco, CA. Check current conditions in San Francisco, CA with radar, hourly, and more.'}]\u001b[0m\u001b[32;1m\u001b[1;3mYou can check the current weather in San Francisco by visiting the [NOAA National Weather Service website](https://forecast.weather.gov/zipcity.php?inputstring=San+Francisco,CA) or [The Weather Channel website](https://weather.com/weather/tenday/l/San Francisco CA USCA0987:1:US).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sf_weather = agent_executor.invoke({\"input\": \"what is the weather in SF?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can check the current weather in San Francisco by visiting the [NOAA\n",
      "National Weather Service\n",
      "website](https://forecast.weather.gov/zipcity.php?inputstring=San+Francisco,CA)\n",
      "or [The Weather Channel website](https://weather.com/weather/tenday/l/San\n",
      "Francisco CA USCA0987:1:US).\n"
     ]
    }
   ],
   "source": [
    "printlongtext(sf_weather[\"output\"], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith_search` with `{'query': 'LangSmith LLM application testing'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mLangSmith | 🦜️🛠️ LangSmith\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to main content🦜️🛠️ LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubLangSmithOn this pageLangSmithIntroduction​LangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the go-to open source framework for building with LLMs.LangSmith is developed by LangChain, the company behind the open source LangChain framework.Quick Start​Tracing: Get started with the tracing quick start.Evaluation: Get started with the evaluation quick start.Next Steps​Check out the following sections to learn more about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Setup: Learn how to create an account, obtain an API key, and configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources​LangSmith Cookbook: A collection of tutorials and end-to-end walkthroughs using LangSmith.LangChain Python: Docs for the Python LangChain library.LangChain Python API Reference: documentation to review the core APIs of LangChain.LangChain JS: Docs for the TypeScript LangChain libraryDiscord: Join us on our Discord to discuss all things LangChain!Contact SalesIf you're interested in enterprise security and admin features, special deployment options, or access for large teams, reach out to speak with sales.NextUser GuideIntroductionQuick StartNext StepsAdditional ResourcesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright © 2024 LangChain, Inc.\u001b[0m\u001b[32;1m\u001b[1;3mLangSmith is a platform for building production-grade LLM applications. It allows you to debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework. LangSmith seamlessly integrates with LangChain, an open-source framework for building with LLMs. You can learn more about LangSmith by checking out their user guide, setup instructions, pricing, self-hosting options, tracing capabilities, evaluation capabilities, and the Prompt Hub. If you are interested in testing your LLM applications, LangSmith can help you with that process.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "result_ls = agent_executor.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is a platform for building production-grade LLM applications. It\n",
      "allows you to debug, test, evaluate, and monitor chains and intelligent agents\n",
      "built on any LLM framework. LangSmith seamlessly integrates with LangChain, an\n",
      "open-source framework for building with LLMs. You can learn more about LangSmith\n",
      "by checking out their user guide, setup instructions, pricing, self-hosting\n",
      "options, tracing capabilities, evaluation capabilities, and the Prompt Hub. If\n",
      "you are interested in testing your LLM applications, LangSmith can help you with\n",
      "that process.\n"
     ]
    }
   ],
   "source": [
    "printlongtext(result_ls[\"output\"], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving with LangServe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to serv.py\n",
    "\n",
    "Also needs to: `pip install \"langserve[all]\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangServe client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_chain = RemoteRunnable(\"http://localhost:8080/agent/\")\n",
    "result =remote_chain.invoke({\n",
    "    \"input\": \"how can langsmith help with testing?\",\n",
    "    \"chat_history\": []  # Providing an empty list as this is the first call\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is a platform for building production-grade LLM (Large Language Model) applications. It can help with testing by providing capabilities for debugging, testing, evaluating, and monitoring chains and intelligent agents built on any LLM framework. LangSmith seamlessly integrates with LangChain, an open-source framework for building with LLMs. \n",
      "\n",
      "Some ways LangSmith can help with testing include:\n",
      "- Tracing: LangSmith offers tracing capabilities to help you trace and debug your LLM applications.\n",
      "- Evaluation: LangSmith provides evaluation capabilities to assess the performance and effectiveness of your LLM models during testing.\n",
      "\n",
      "You can learn more about how LangSmith can assist with testing by exploring its user guide, setup instructions, pricing, and additional resources available on the LangSmith platform.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"output\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
